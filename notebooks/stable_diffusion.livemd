# Stable Diffusion

```elixir
Mix.install([
  {:bumblebee, path: Path.expand("..", __DIR__)},
  {:nx, github: "elixir-nx/nx", sparse: "nx", override: true},
  {:exla, github: "elixir-nx/nx", sparse: "exla", override: true},
  {:stb_image, "~> 0.5.0"},
  {:kino, "~> 0.7.0"}
])

Nx.global_default_backend(EXLA.Backend)
```

## Introduction

Stable Diffusion is a latent text-to-image diffusion model, primarily used to generate images based on a text prompt. Ever since it [became open-source](https://stability.ai/blog/stable-diffusion-public-release), the research, applications and tooling around it exploded. You can find a ton of resources and examples online, meanwhile let's see how to run Stable Diffusion using Bumblebee!

<!-- livebook:{"branch_parent_index":0} -->

## Text to image

Stable Diffusion is composed of several separate models and preprocessors, so we will load all of them.

In order to download the models, you need to sign up on Hugging Face, accept the license agreement at [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4), generate an access token in [the settings](https://huggingface.co/settings/tokens) and put it in `HF_TOKEN` Livebook secret.

```elixir
auth_token = System.fetch_env!("LB_HF_TOKEN")

{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "openai/clip-vit-large-patch14"})

{:ok, clip} =
  Bumblebee.load_model(
    {:hf, "CompVis/stable-diffusion-v1-4", auth_token: auth_token, subdir: "text_encoder"}
  )

{:ok, vae} =
  Bumblebee.load_model(
    {:hf, "CompVis/stable-diffusion-v1-4", auth_token: auth_token, subdir: "vae"},
    architecture: :decoder,
    params_filename: "diffusion_pytorch_model.bin"
  )

{:ok, unet} =
  Bumblebee.load_model(
    {:hf, "CompVis/stable-diffusion-v1-4", auth_token: auth_token, subdir: "unet"},
    params_filename: "diffusion_pytorch_model.bin"
  )

{:ok, scheduler} =
  Bumblebee.load_scheduler(
    {:hf, "CompVis/stable-diffusion-v1-4", auth_token: auth_token, subdir: "scheduler"}
  )

{:ok, featurizer} =
  Bumblebee.load_featurizer(
    {:hf, "CompVis/stable-diffusion-v1-4", auth_token: auth_token, subdir: "feature_extractor"}
  )

{:ok, safety_checker} =
  Bumblebee.load_model(
    {:hf, "CompVis/stable-diffusion-v1-4", auth_token: auth_token, subdir: "safety_checker"}
  )

:ok
```

With all the models loaded, we can now configure a serving implementation of the text-to-image task.

```elixir
serving =
  Bumblebee.Diffusion.StableDiffusion.text_to_image(clip, vae, unet, tokenizer, scheduler,
    num_steps: 20,
    num_images_per_prompt: 2,
    safety_checker: safety_checker,
    safety_checker_featurizer: featurizer,
    compile: [batch_size: 1, sequence_length: 60],
    defn_options: [compiler: EXLA]
  )

text_input =
  Kino.Input.text("Prompt", default: "numbat, forest, high quality, detailed, digital art")
```

Now we are ready to generate the images!

```elixir
prompt = Kino.Input.read(text_input)

output = Nx.Serving.run(serving, prompt)

for result <- output.results do
  result.image
  |> StbImage.from_nx()
  |> StbImage.to_binary(:png)
  |> Kino.Image.new(:png)
end
|> Kino.Layout.grid(columns: 2)
```

*Note: Stable Diffusion is a very involved model, so the generation can take a long time if you run it on a CPU.*

If you have a GPU available, feel free to increase the number of steps and images to achieve a better quality.
